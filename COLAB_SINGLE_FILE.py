#!/usr/bin/env python3
# ============================================
# RAG SaaS Platform - TEK DOSYA BAÅLATMA
# ============================================
# TÃ¼m proje bu dosyada - Colab'te tek hÃ¼crede Ã§alÄ±ÅŸÄ±r
# Backend (FastAPI) + Frontend (Gradio) + RAG Engine hepsi burada
#
# KULLANIM (Colab):
# 1. Colab'te yeni hÃ¼cre oluÅŸtur
# 2. AÅŸaÄŸÄ±daki komutu Ã§alÄ±ÅŸtÄ±r:
#    !wget -q -O - https://raw.githubusercontent.com/mmcanpolat/rag_nlp_chatbotplatform/main/COLAB_SINGLE_FILE.py | python3
#
# VEYA:
# 1. GitHub'dan dosyayÄ± kopyala-yapÄ±ÅŸtÄ±r
# 2. Shift+Enter ile Ã§alÄ±ÅŸtÄ±r
# 3. Public URL terminal Ã§Ä±ktÄ±sÄ±nda gÃ¶rÃ¼necek

# BaÄŸÄ±mlÄ±lÄ±klarÄ± kontrol et ve eksikleri kur
import sys
import subprocess

print("=" * 60)
print("RAG SaaS Platform - Tek Dosya BaÅŸlatma")
print("=" * 60)
print("\n[1/4] BaÄŸÄ±mlÄ±lÄ±klar kontrol ediliyor...")

required_packages = [
    "fastapi", "uvicorn[standard]", "gradio>=4.0.0", "langchain", "langchain-community",
    "langchain-huggingface", "langchain-text-splitters", "langchain-core", "transformers", 
    "torch", "sentence-transformers", "faiss-cpu", "pypdf", "docx2txt", "beautifulsoup4", 
    "requests", "python-dotenv", "rouge-score", "matplotlib", "pandas", "numpy", "seaborn"
]

missing = []
for pkg in required_packages:
    try:
        # Paket adÄ±nÄ± import adÄ±na Ã§evir
        pkg_clean = pkg.split("[")[0].split(">=")[0]
        # Ã–zel durumlar - import adlarÄ±
        if pkg_clean == "langchain-text-splitters":
            pkg_import = "langchain_text_splitters"
        elif pkg_clean == "langchain-core":
            pkg_import = "langchain_core"
        elif pkg_clean == "langchain-community":
            pkg_import = "langchain_community"
        elif pkg_clean == "langchain-huggingface":
            pkg_import = "langchain_huggingface"
        elif pkg_clean == "sentence-transformers":
            pkg_import = "sentence_transformers"
        elif pkg_clean == "python-dotenv":
            pkg_import = "dotenv"
        elif pkg_clean == "faiss-cpu":
            pkg_import = "faiss"
        elif pkg_clean == "beautifulsoup4":
            pkg_import = "bs4"
        else:
            pkg_import = pkg_clean.replace("-", "_")
        __import__(pkg_import)
    except ImportError:
        missing.append(pkg)

if missing:
    print(f"âš ï¸  {len(missing)} paket eksik, kuruluyor...")
    print(f"   Eksik paketler: {', '.join(missing)}")
    print("   (Bu iÅŸlem 5-10 dakika sÃ¼rebilir...)\n")
    
    # Eksik paketleri kur
    try:
        subprocess.run(
            [sys.executable, "-m", "pip", "install", "-q", "--upgrade"] + missing,
            check=False,
            timeout=600  # 10 dakika timeout
        )
        print("âœ… Eksik paketler kuruldu")
        
        # Tekrar kontrol et
        still_missing = []
        for pkg in missing:
            try:
                pkg_clean = pkg.split("[")[0].split(">=")[0]
                if pkg_clean == "langchain-text-splitters":
                    pkg_import = "langchain_text_splitters"
                elif pkg_clean == "langchain-core":
                    pkg_import = "langchain_core"
                elif pkg_clean == "langchain-community":
                    pkg_import = "langchain_community"
                elif pkg_clean == "langchain-huggingface":
                    pkg_import = "langchain_huggingface"
                elif pkg_clean == "sentence-transformers":
                    pkg_import = "sentence_transformers"
                elif pkg_clean == "python-dotenv":
                    pkg_import = "dotenv"
                elif pkg_clean == "faiss-cpu":
                    pkg_import = "faiss"
                elif pkg_clean == "beautifulsoup4":
                    pkg_import = "bs4"
                else:
                    pkg_import = pkg_clean.replace("-", "_")
                __import__(pkg_import)
            except ImportError:
                still_missing.append(pkg)
        
        if still_missing:
            print(f"âŒ Hala eksik paketler var: {', '.join(still_missing)}")
            print("LÃ¼tfen manuel olarak kurun:")
            print(f"  !pip install {' '.join(still_missing)}")
            sys.exit(1)
        else:
            print("âœ… TÃ¼m baÄŸÄ±mlÄ±lÄ±klar kuruldu ve kontrol edildi")
    except Exception as e:
        print(f"âŒ Paket kurulumu sÄ±rasÄ±nda hata: {e}")
        print("LÃ¼tfen manuel olarak kurun:")
        print(f"  !pip install {' '.join(missing)}")
        sys.exit(1)
else:
    print("âœ… TÃ¼m baÄŸÄ±mlÄ±lÄ±klar mevcut")

print("\n[2/4] ModÃ¼ller yÃ¼kleniyor...")

import os
import json
import time
import secrets
import threading
import re
from pathlib import Path
from typing import Optional, Dict, List, Tuple
from datetime import datetime
from urllib.parse import urlparse

# FastAPI ve Gradio import'larÄ±
from fastapi import FastAPI, HTTPException, Depends, UploadFile, File, Form, Header
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import JSONResponse
from pydantic import BaseModel
import uvicorn
import gradio as gr

# LangChain ve Hugging Face
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS
from langchain_community.document_loaders import (
    PyPDFLoader, WebBaseLoader, Docx2txtLoader, TextLoader, JSONLoader, CSVLoader
)
# LangChain text splitter - yeni versiyonlarda ayrÄ± pakette
try:
    from langchain_text_splitters import RecursiveCharacterTextSplitter
except ImportError:
    try:
        from langchain.text_splitter import RecursiveCharacterTextSplitter
    except ImportError:
        # En son Ã§are: langchain paketinden
        from langchain.text_splitter import RecursiveCharacterTextSplitter

# Document - yeni versiyonlarda langchain_core'da
try:
    from langchain_core.documents import Document
except ImportError:
    from langchain.schema import Document
from transformers import GPT2LMHeadModel, GPT2Tokenizer
import torch

print("âœ… ModÃ¼ller yÃ¼klendi")

# ==================== CONFIG ====================
BASE_DIR = Path.cwd()
# Colab'te git clone sonrasÄ± dizin yapÄ±sÄ±
if "rag_nlp_chatbotplatform" in str(BASE_DIR):
    BASE_DIR = BASE_DIR / "rag_nlp_chatbotplatform"

DATA_DIR = BASE_DIR / "python_services" / "data"
INDEX_DIR = DATA_DIR / "faiss_index"
UPLOADS_DIR = DATA_DIR / "uploads"

# Dizinleri oluÅŸtur
for d in [DATA_DIR, INDEX_DIR, UPLOADS_DIR, BASE_DIR / "frontend_gradio" / "assets" / "plots"]:
    d.mkdir(parents=True, exist_ok=True)

print("\n[3/5] YapÄ±landÄ±rma tamamlandÄ±")

# ==================== IN-MEMORY DATA ====================
companies: Dict[str, dict] = {}
agents: Dict[str, dict] = {}
sessions: Dict[str, dict] = {}

SUPER_ADMIN = {
    "id": "superadmin",
    "username": "admin@ragplatform.com",
    "password": "Admin123!@#",
    "isSuperAdmin": True,
    "companyId": "superadmin",
    "companyName": "SuperAdmin"
}

# ==================== HELPER FUNCTIONS ====================
def gen_id() -> str:
    return f"{int(time.time() * 1000)}_{secrets.token_hex(4)}"

def generate_strong_password() -> str:
    return secrets.token_urlsafe(24)

# ==================== DOCUMENT INGESTOR ====================
class DocumentIngestor:
    def __init__(self, index_name: str = "default", embedding_model: str = None):
        self.index_name = index_name
        self.index_path = INDEX_DIR / index_name
        self.embedding_model_name = embedding_model or "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2"
        self.index_path.mkdir(parents=True, exist_ok=True)
        self._embeddings = None
        self.text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=750,
            chunk_overlap=100,
            separators=["\n\n", "\n", ".", "!", "?", ";", ":", " ", ""],
            length_function=len
        )
    
    @property
    def embeddings(self):
        if self._embeddings is None:
            device = 'cuda' if torch.cuda.is_available() else 'cpu'
            self._embeddings = HuggingFaceEmbeddings(
                model_name=self.embedding_model_name,
                model_kwargs={'device': device},
                encode_kwargs={'normalize_embeddings': True}
            )
        return self._embeddings
    
    def detect_source_type(self, source: str) -> str:
        parsed = urlparse(source)
        if parsed.scheme in ('http', 'https'):
            return 'web'
        ext = Path(source).suffix.lower()
        return {
            '.pdf': 'pdf',
            '.docx': 'docx',
            '.txt': 'text',
            '.json': 'json',
            '.csv': 'csv'
        }.get(ext, 'text')
    
    def load_document(self, source: str) -> List[Document]:
        source_type = self.detect_source_type(source)
        try:
            if source_type == 'web':
                loader = WebBaseLoader(source)
            elif source_type == 'pdf':
                loader = PyPDFLoader(source)
            elif source_type == 'docx':
                loader = Docx2txtLoader(source)
            elif source_type == 'json':
                loader = JSONLoader(source, jq_schema='.')
            elif source_type == 'csv':
                loader = CSVLoader(source)
            else:
                loader = TextLoader(source)
            return loader.load()
        except Exception as e:
            print(f"[!] YÃ¼kleme hatasÄ±: {e}")
            return []
    
    def ingest(self, source: str, progress_callback=None) -> dict:
        try:
            if progress_callback:
                progress_callback("DÃ¶kÃ¼man yÃ¼kleniyor...", 0, 0)
            
            docs = self.load_document(source)
            if not docs:
                return {"success": False, "error": "DÃ¶kÃ¼man yÃ¼klenemedi"}
            
            if progress_callback:
                progress_callback(f"DÃ¶kÃ¼man yÃ¼klendi ({len(docs)} sayfa)", 0, 0)
            
            # CSV dosyalarÄ± iÃ§in: Her satÄ±rÄ± direkt chunk yap (text splitter kullanma)
            source_type = self.detect_source_type(source)
            if source_type == 'csv':
                if progress_callback:
                    progress_callback(f"CSV satÄ±rlarÄ± direkt embed edilecek ({len(docs)} satÄ±r)...", 0, 0)
                # CSV iÃ§in: Her Document (satÄ±r) direkt bir chunk
                chunks = docs  # Text splitter kullanmadan direkt kullan
            else:
                # DiÄŸer dosya tipleri iÃ§in text splitter kullan
                if progress_callback:
                    progress_callback("DÃ¶kÃ¼man parÃ§alara bÃ¶lÃ¼nÃ¼yor...", 0, 0)
                chunks = self.text_splitter.split_documents(docs)
            
            if not chunks:
                return {"success": False, "error": "DÃ¶kÃ¼man bÃ¶lÃ¼nemedi"}
            
            total_chunks = len(chunks)
            if progress_callback:
                progress_callback(f"{total_chunks} parÃ§a oluÅŸturuldu. Embedding baÅŸlÄ±yor...", 0, total_chunks)
            
            # Mevcut index varsa merge et, yoksa yeni oluÅŸtur
            if (self.index_path / "index.faiss").exists():
                if progress_callback:
                    progress_callback("Mevcut index yÃ¼kleniyor...", 0, total_chunks)
                vectorstore = FAISS.load_local(
                    str(self.index_path),
                    self.embeddings,
                    allow_dangerous_deserialization=True
                )
                
                # Batch'ler halinde ekle (progress iÃ§in)
                batch_size = 100
                total_batches = (len(chunks) + batch_size - 1) // batch_size
                for i in range(0, len(chunks), batch_size):
                    batch = chunks[i:i+batch_size]
                    vectorstore.add_documents(batch)
                    batch_num = (i // batch_size) + 1
                    percent = int(((i + len(batch)) / total_chunks) * 100) if total_chunks > 0 else 0
                    if progress_callback:
                        progress_callback(
                            f"Batch {batch_num}/{total_batches} iÅŸleniyor ({len(batch)} parÃ§a) - %{percent}",
                            i + len(batch),
                            total_chunks
                        )
            else:
                # Yeni index oluÅŸtur - batch'ler halinde
                batch_size = 100
                total_batches = (len(chunks) + batch_size - 1) // batch_size
                first_batch = chunks[:batch_size]
                percent = int((len(first_batch) / total_chunks) * 100) if total_chunks > 0 else 0
                if progress_callback:
                    progress_callback(f"Ä°lk batch oluÅŸturuluyor ({len(first_batch)} parÃ§a) - %{percent}", len(first_batch), total_chunks)
                
                vectorstore = FAISS.from_documents(first_batch, self.embeddings)
                
                # Kalan batch'leri ekle
                for i in range(batch_size, len(chunks), batch_size):
                    batch = chunks[i:i+batch_size]
                    vectorstore.add_documents(batch)
                    batch_num = (i // batch_size) + 1
                    percent = int(((i + len(batch)) / total_chunks) * 100) if total_chunks > 0 else 0
                    if progress_callback:
                        progress_callback(
                            f"Batch {batch_num + 1}/{total_batches} iÅŸleniyor ({len(batch)} parÃ§a) - %{percent}",
                            i + len(batch),
                            total_chunks
                        )
            
            if progress_callback:
                progress_callback("Index kaydediliyor...", total_chunks, total_chunks)
            
            vectorstore.save_local(str(self.index_path))
            
            # Metadata kaydet
            with open(self.index_path / "metadata.json", "w") as f:
                json.dump({
                    "embedding_model": self.embedding_model_name,
                    "created_at": datetime.now().isoformat(),
                    "chunk_count": total_chunks
                }, f)
            
            if progress_callback:
                progress_callback(f"âœ… TamamlandÄ±! {total_chunks} parÃ§a iÅŸlendi.", total_chunks, total_chunks)
            
            return {"success": True, "chunks": total_chunks}
        except Exception as e:
            if progress_callback:
                progress_callback(f"âŒ Hata: {str(e)}", 0, 0)
            return {"success": False, "error": str(e)}

# ==================== RAG ENGINE ====================
class SimpleRAGEngine:
    def __init__(self, index_name: str = "default"):
        self.index_name = index_name
        self.index_path = INDEX_DIR / index_name
        self.vectorstore = None
        self._embeddings = None
        self._gpt_model = None
        self._gpt_tokenizer = None
        self._load_vectorstore()
    
    @property
    def embeddings(self):
        if self._embeddings is None:
            device = 'cuda' if torch.cuda.is_available() else 'cpu'
            meta_file = self.index_path / "metadata.json"
            model_name = "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2"
            if meta_file.exists():
                try:
                    with open(meta_file, 'r') as f:
                        meta = json.load(f)
                        model_name = meta.get('embedding_model', model_name)
                except:
                    pass
            self._embeddings = HuggingFaceEmbeddings(
                model_name=model_name,
                model_kwargs={'device': device},
                encode_kwargs={'normalize_embeddings': True}
            )
        return self._embeddings
    
    def _load_vectorstore(self):
        index_file = self.index_path / "index.faiss"
        if index_file.exists():
            try:
                self.vectorstore = FAISS.load_local(
                    str(self.index_path),
                    self.embeddings,
                    allow_dangerous_deserialization=True
                )
            except:
                pass
    
    def _load_gpt_model(self):
        if self._gpt_model is None:
            print("[*] TÃ¼rkÃ§e GPT-2 modeli yÃ¼kleniyor...")
            device = 'cuda' if torch.cuda.is_available() else 'cpu'
            
            # TÃ¼rkÃ§e GPT-2 modelleri - sÄ±rayla dene
            turkish_models = [
                "redrussianarmy/gpt2-turkish-cased",  # En yaygÄ±n TÃ¼rkÃ§e GPT-2
                "gorkemgoknar/gpt2-small-turkish",    # Alternatif TÃ¼rkÃ§e model
                "cenkersisman/gpt2-turkish-256-token" # BaÅŸka bir TÃ¼rkÃ§e model
            ]
            
            model_loaded = False
            for model_name in turkish_models:
                try:
                    print(f"[*] Model deneniyor: {model_name}...")
                    self._gpt_tokenizer = GPT2Tokenizer.from_pretrained(model_name)
                    self._gpt_model = GPT2LMHeadModel.from_pretrained(model_name)
                    print(f"[*] âœ… TÃ¼rkÃ§e GPT-2 modeli yÃ¼klendi: {model_name}")
                    model_loaded = True
                    break
                except Exception as e:
                    print(f"[!] {model_name} yÃ¼klenemedi: {str(e)[:100]}")
                    continue
            
            if not model_loaded:
                print("[!] HiÃ§bir TÃ¼rkÃ§e model yÃ¼klenemedi, Ä°ngilizce GPT-2 kullanÄ±lÄ±yor...")
                model_name = "gpt2"
                self._gpt_tokenizer = GPT2Tokenizer.from_pretrained(model_name)
                self._gpt_model = GPT2LMHeadModel.from_pretrained(model_name)
                print(f"[*] âš ï¸ Ä°ngilizce GPT-2 modeli yÃ¼klendi (fallback): {model_name}")
            
            self._gpt_model.to(device)
            self._gpt_model.eval()
            if self._gpt_tokenizer.pad_token is None:
                self._gpt_tokenizer.pad_token = self._gpt_tokenizer.eos_token
    
    def retrieve(self, query: str, top_k: int = 3) -> List[Dict]:
        if not self.vectorstore:
            return []
        results = self.vectorstore.similarity_search_with_score(query, k=top_k)
        return [{'context': doc.page_content, 'score': float(1 - score)} 
                for doc, score in results]
    
    def _ask_gpt(self, query: str, contexts: List[str]) -> Tuple[str, float]:
        try:
            self._load_gpt_model()
            if self._gpt_model is None:
                return "Model yÃ¼klenemedi", 0.0
            
            # Context'i hazÄ±rla - her context'i ayrÄ± satÄ±r olarak
            context_text = "\n\n".join([doc for doc in contexts])
            
            # Prompt yapÄ±sÄ±: Context + Soru formatÄ± (kullanÄ±cÄ±nÄ±n istediÄŸi format)
            prompt = f"""AÅŸaÄŸÄ±daki tÄ±bbi soru-cevap geÃ§miÅŸine dayanarak hastanÄ±n sorusunu cevapla.
EÄŸer verilen context iÃ§inde cevap yoksa, "Bilmiyorum" de.

CONTEXT:
{context_text}

SORU:
{query}

CEVAP:"""
            
            device = next(self._gpt_model.parameters()).device
            inputs = self._gpt_tokenizer.encode(prompt, return_tensors="pt", max_length=1024, truncation=True)
            inputs = inputs.to(device)
            
            with torch.no_grad():
                outputs = self._gpt_model.generate(
                    inputs,
                    max_length=inputs.shape[1] + 150,
                    min_length=inputs.shape[1] + 20,
                    temperature=0.7,
                    do_sample=True,
                    pad_token_id=self._gpt_tokenizer.eos_token_id,
                    eos_token_id=self._gpt_tokenizer.eos_token_id,
                    repetition_penalty=1.2,
                    no_repeat_ngram_size=3
                )
            
            generated = outputs[0][inputs.shape[1]:]
            answer = self._gpt_tokenizer.decode(generated, skip_special_tokens=True).strip()
            
            # EÄŸer cevap Ã§ok kÄ±sa veya anlamsÄ±zsa, context'ten direkt al
            if len(answer) < 15 or "bilmiyorum" in answer.lower() or answer.lower().startswith("bilmiyorum"):
                # Context'lerden en alakalÄ± olanÄ± kullan (ilk context en yÃ¼ksek skorlu)
                if contexts:
                    # Ä°lk context'i kullan (en yÃ¼ksek skorlu)
                    answer = contexts[0] if len(contexts[0]) < 500 else contexts[0][:500] + "..."
                    conf = 0.7
                else:
                    answer = "Bilgi bulunamadÄ±."
                    conf = 0.3
            else:
                conf = min(0.85, 0.5 + len(answer) / 200)
            
            return answer, conf
        except Exception as e:
            import traceback
            print(f"[!] GPT model hatasÄ±: {traceback.format_exc()}")
            # Fallback: Context'ten direkt cevap ver
            if contexts:
                return contexts[0][:300] + "..." if len(contexts[0]) > 300 else contexts[0], 0.6
            return f"Hata: {str(e)}", 0.0
    
    def query(self, text: str, model_type: str = "GPT") -> Dict:
        start = time.time()
        retrieved = self.retrieve(text, top_k=3)
        contexts = [r['context'] for r in retrieved]
        
        if not contexts:
            return {
                "answer": "Ã–nce veri yÃ¼kleyin. Agent oluÅŸtururken dosya veya URL ekleyin.",
                "context": "",
                "confidence": 0.0,
                "model_used": "GPT",
                "response_time_ms": round((time.time() - start) * 1000, 2)
            }
        
        if model_type.upper() == "GPT":
            answer, conf = self._ask_gpt(text, contexts)
        else:
            answer = contexts[0][:200] + "..."
            conf = 0.7
        
        return {
            "answer": answer,
            "context": contexts[0] if contexts else "",
            "confidence": round(conf, 4),
            "model_used": model_type,
            "response_time_ms": round((time.time() - start) * 1000, 2)
        }

# ==================== EVALUATION METRICS ====================
class MetricsEvaluator:
    """Evaluation metrikleri hesaplama: Exact Match, F1, ROUGE-L, Cosine Similarity"""
    
    def __init__(self):
        from sentence_transformers import SentenceTransformer
        from rouge_score import rouge_scorer
        self.embed_model = SentenceTransformer("sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2")
        self.rouge_scorer = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)
    
    def exact_match(self, pred: str, ref: str) -> float:
        """Exact Match: Harfi harfine aynÄ± mÄ±?"""
        return 1.0 if pred.strip().lower() == ref.strip().lower() else 0.0
    
    def f1_score(self, pred: str, ref: str) -> float:
        """Token-based F1 Score"""
        pred_tokens = set(pred.lower().split())
        ref_tokens = set(ref.lower().split())
        
        if not pred_tokens or not ref_tokens:
            return 0.0
        
        common = pred_tokens & ref_tokens
        if not common:
            return 0.0
        
        precision = len(common) / len(pred_tokens)
        recall = len(common) / len(ref_tokens)
        
        if precision + recall == 0:
            return 0.0
        
        return 2 * precision * recall / (precision + recall)
    
    def rouge_l(self, pred: str, ref: str) -> float:
        """ROUGE-L F1 Score"""
        scores = self.rouge_scorer.score(ref, pred)
        return scores['rougeL'].fmeasure
    
    def cosine_similarity(self, pred: str, ref: str) -> float:
        """Cosine Similarity - embedding bazlÄ±"""
        vecs = self.embed_model.encode([pred, ref])
        import numpy as np
        return float(np.dot(vecs[0], vecs[1]) / (np.linalg.norm(vecs[0]) * np.linalg.norm(vecs[1])))
    
    def evaluate(self, predictions: List[str], references: List[str]) -> Dict:
        """TÃ¼m metrikleri hesapla"""
        if len(predictions) != len(references):
            return {"error": "Predictions ve references aynÄ± uzunlukta olmalÄ±"}
        
        em_scores = []
        f1_scores = []
        rouge_scores = []
        cosine_scores = []
        
        for pred, ref in zip(predictions, references):
            em_scores.append(self.exact_match(pred, ref))
            f1_scores.append(self.f1_score(pred, ref))
            rouge_scores.append(self.rouge_l(pred, ref))
            cosine_scores.append(self.cosine_similarity(pred, ref))
        
        return {
            "exact_match": {
                "scores": em_scores,
                "mean": sum(em_scores) / len(em_scores) if em_scores else 0.0
            },
            "f1": {
                "scores": f1_scores,
                "mean": sum(f1_scores) / len(f1_scores) if f1_scores else 0.0
            },
            "rouge_l": {
                "scores": rouge_scores,
                "mean": sum(rouge_scores) / len(rouge_scores) if rouge_scores else 0.0
            },
            "cosine_similarity": {
                "scores": cosine_scores,
                "mean": sum(cosine_scores) / len(cosine_scores) if cosine_scores else 0.0
            }
        }
    
    def plot_metrics(self, results: Dict[str, Dict], save_path: str = None):
        """Metrikleri gÃ¶rselleÅŸtir"""
        import matplotlib.pyplot as plt
        import pandas as pd
        
        models = list(results.keys())
        metrics_data = {
            "Exact Match": [results[m]["exact_match"]["mean"] for m in models],
            "F1 Score": [results[m]["f1"]["mean"] for m in models],
            "ROUGE-L": [results[m]["rouge_l"]["mean"] for m in models],
            "Cosine Similarity": [results[m]["cosine_similarity"]["mean"] for m in models]
        }
        
        df = pd.DataFrame(metrics_data, index=models)
        
        fig, axes = plt.subplots(2, 2, figsize=(14, 10))
        fig.suptitle("Model Performance Metrics Comparison", fontsize=16, fontweight="bold")
        
        # Bar chart - TÃ¼m metrikler
        ax1 = axes[0, 0]
        df.plot(kind="bar", ax=ax1, width=0.8)
        ax1.set_title("All Metrics Comparison")
        ax1.set_ylabel("Score")
        ax1.set_xlabel("Model")
        ax1.legend(loc="best")
        ax1.grid(axis="y", alpha=0.3)
        ax1.set_ylim(0, 1)
        
        # Line chart
        ax2 = axes[0, 1]
        df.plot(kind="line", ax=ax2, marker="o", linewidth=2, markersize=8)
        ax2.set_title("Metrics Trend")
        ax2.set_ylabel("Score")
        ax2.set_xlabel("Model")
        ax2.legend(loc="best")
        ax2.grid(alpha=0.3)
        ax2.set_ylim(0, 1)
        
        # Heatmap
        ax3 = axes[1, 0]
        import seaborn as sns
        sns.heatmap(df.T, annot=True, fmt=".3f", cmap="YlOrRd", ax=ax3, cbar_kws={"label": "Score"})
        ax3.set_title("Metrics Heatmap")
        ax3.set_ylabel("Metric")
        ax3.set_xlabel("Model")
        
        # Radar chart (spider chart)
        ax4 = axes[1, 1]
        angles = np.linspace(0, 2 * np.pi, len(df.columns), endpoint=False).tolist()
        angles += angles[:1]  # Kapat
        
        ax4 = plt.subplot(2, 2, 4, projection="polar")
        for idx, model in enumerate(models):
            values = [df.loc[model, col] for col in df.columns]
            values += values[:1]  # Kapat
            ax4.plot(angles, values, "o-", linewidth=2, label=model)
            ax4.fill(angles, values, alpha=0.25)
        
        ax4.set_xticks(angles[:-1])
        ax4.set_xticklabels(df.columns)
        ax4.set_ylim(0, 1)
        ax4.set_title("Radar Chart", pad=20)
        ax4.legend(loc="upper right", bbox_to_anchor=(1.3, 1.1))
        ax4.grid(True)
        
        plt.tight_layout()
        
        if save_path:
            plt.savefig(save_path, dpi=300, bbox_inches="tight")
            print(f"[*] Grafik kaydedildi: {save_path}")
        
        return fig

# ==================== FASTAPI BACKEND ====================
backend_app = FastAPI(title="RAG SaaS Platform API")
backend_app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Pydantic Models
class LoginRequest(BaseModel):
    username: str
    password: str

class CompanyCreate(BaseModel):
    name: str
    email: Optional[str] = None
    phone: Optional[str] = None
    description: Optional[str] = None

class AgentCreate(BaseModel):
    name: str
    embedding_model: str
    data_source_type: str
    data_source: Optional[str] = None

class ChatRequest(BaseModel):
    agent_id: str
    query: str
    model: str = "GPT"

# Auth
def get_auth_header(authorization: Optional[str] = Header(None)):
    return authorization

def require_auth(authorization: Optional[str] = Depends(get_auth_header)):
    if not authorization or not authorization.startswith("Bearer "):
        raise HTTPException(status_code=401, detail="Unauthorized")
    token = authorization.split(" ")[1]
    if token not in sessions:
        raise HTTPException(status_code=401, detail="Invalid token")
    return sessions[token]

def require_superadmin(user: dict = Depends(require_auth)):
    if not user.get("isSuperAdmin"):
        raise HTTPException(status_code=403, detail="SuperAdmin required")
    return user

# Endpoints
@backend_app.post("/api/auth/login")
async def login(req: LoginRequest):
    if req.username == SUPER_ADMIN["username"] and req.password == SUPER_ADMIN["password"]:
        token = secrets.token_urlsafe(32)
        sessions[token] = SUPER_ADMIN
        return {"success": True, "token": token, "data": SUPER_ADMIN}
    
    for company in companies.values():
        if company.get("username") == req.username and company.get("password") == req.password:
            token = secrets.token_urlsafe(32)
            user = {
                "id": company["id"],
                "username": company["username"],
                "companyId": company["id"],
                "companyName": company["name"],
                "isSuperAdmin": False
            }
            sessions[token] = user
            return {"success": True, "token": token, "data": user}
    
    raise HTTPException(status_code=401, detail="Invalid credentials")

@backend_app.post("/api/admin/companies")
async def create_company(company: CompanyCreate, user: dict = Depends(require_superadmin)):
    company_id = gen_id()
    username = f"{company.name.lower().replace(' ', '_')}@company.com"
    password = generate_strong_password()
    
    new_company = {
        "id": company_id,
        "name": company.name,
        "email": company.email or username,
        "phone": company.phone,
        "description": company.description,
        "username": username,
        "password": password,
        "createdAt": datetime.now().isoformat()
    }
    companies[company_id] = new_company
    return {"success": True, "data": new_company}

@backend_app.get("/api/admin/companies")
async def list_companies(user: dict = Depends(require_superadmin)):
    return {"success": True, "data": list(companies.values())}

@backend_app.post("/api/agents")
async def create_agent(agent: AgentCreate, user: dict = Depends(require_auth)):
    agent_id = gen_id()
    index_name = f"agent_{agent_id}"
    
    new_agent = {
        "id": agent_id,
        "name": agent.name,
        "companyId": user.get("companyId"),
        "embeddingModel": agent.embedding_model,
        "dataSourceType": agent.data_source_type,
        "dataSource": agent.data_source,
        "indexName": index_name,
        "createdAt": datetime.now().isoformat()
    }
    
    # Ingestion yap (progress callback ile detaylÄ± log)
    ingestion_info = {"chunks": 0, "batches": 0, "status": "baÅŸlatÄ±lÄ±yor"}
    if agent.data_source:
        try:
            def progress_log(message, current, total):
                if total > 0:
                    percent = int((current / total) * 100)
                    batch_info = ""
                    if "Batch" in message:
                        # Batch bilgisini parse et
                        import re
                        batch_match = re.search(r'Batch (\d+)/(\d+)', message)
                        if batch_match:
                            batch_num = batch_match.group(1)
                            total_batches = batch_match.group(2)
                            ingestion_info["batches"] = int(total_batches)
                            batch_info = f" (Batch {batch_num}/{total_batches})"
                    print(f"[Ingestion] {message} ({percent}%){batch_info}")
                    ingestion_info["status"] = message
                    ingestion_info["chunks"] = current
                else:
                    print(f"[Ingestion] {message}")
                    ingestion_info["status"] = message
            
            print(f"[*] Agent oluÅŸturuluyor: {agent.name}")
            print(f"[*] Veri kaynaÄŸÄ±: {agent.data_source}")
            print(f"[*] Embedding modeli: {agent.embedding_model}")
            print(f"[*] Index adÄ±: {index_name}")
            print(f"[*] Ingestion baÅŸlÄ±yor...")
            
            ingestor = DocumentIngestor(index_name=index_name, embedding_model=agent.embedding_model)
            result = ingestor.ingest(agent.data_source, progress_callback=progress_log)
            
            if not result.get("success"):
                print(f"[!] Ingestion baÅŸarÄ±sÄ±z: {result.get('error')}")
                return JSONResponse(
                    status_code=400,
                    content={"success": False, "error": result.get("error", "Ingestion failed")}
                )
            
            chunks = result.get("chunks", 0)
            print(f"[+] Ingestion tamamlandÄ±: {chunks} parÃ§a iÅŸlendi")
            new_agent["chunkCount"] = chunks
        except Exception as e:
            import traceback
            print(f"[!] Ingestion hatasÄ±: {traceback.format_exc()}")
            return JSONResponse(
                status_code=400,
                content={"success": False, "error": str(e)}
            )
    
    agents[agent_id] = new_agent
    print(f"[+] Agent oluÅŸturuldu: {agent.name} (ID: {agent_id})")
    return {"success": True, "data": new_agent, "ingestion_info": ingestion_info}

@backend_app.get("/api/agents")
async def list_agents(user: dict = Depends(require_auth)):
    user_agents = [a for a in agents.values() 
                   if a.get("companyId") == user.get("companyId") or user.get("isSuperAdmin")]
    return {"success": True, "data": user_agents}

@backend_app.post("/api/chat")
async def chat(req: ChatRequest, user: dict = Depends(require_auth)):
    try:
        # Sabit index kullan (default)
        index_name = "default"
        
        # EÄŸer agent_id varsa ve geÃ§erliyse onu kullan, yoksa default kullan
        if req.agent_id and req.agent_id in agents:
            agent = agents[req.agent_id]
            if not user.get("isSuperAdmin") and agent.get("companyId") != user.get("companyId"):
                raise HTTPException(status_code=403, detail="Unauthorized")
            index_name = agent.get("indexName", f"agent_{req.agent_id}")
        
        # RAG engine'i oluÅŸtur ve sorguyu Ã§alÄ±ÅŸtÄ±r
        try:
            rag = SimpleRAGEngine(index_name=index_name)
            result = rag.query(req.query, model_type=req.model)
            return {"success": True, "data": result}
        except Exception as e:
            import traceback
            error_detail = traceback.format_exc()
            print(f"[!] RAG Engine hatasÄ±: {error_detail}")
            return JSONResponse(
                status_code=500,
                content={
                    "success": False,
                    "error": f"RAG Engine hatasÄ±: {str(e)}",
                    "detail": error_detail
                }
            )
    except HTTPException:
        raise
    except Exception as e:
        import traceback
        error_detail = traceback.format_exc()
        print(f"[!] Chat endpoint hatasÄ±: {error_detail}")
        return JSONResponse(
            status_code=500,
            content={
                "success": False,
                "error": f"Sunucu hatasÄ±: {str(e)}",
                "detail": error_detail
            }
        )

@backend_app.post("/api/upload")
async def upload_file(file: UploadFile = File(...), user: dict = Depends(require_auth)):
    file_path = UPLOADS_DIR / f"upload_{int(time.time() * 1000)}_{file.filename}"
    with open(file_path, "wb") as f:
        content = await file.read()
        f.write(content)
    return {"success": True, "data": {"filePath": str(file_path), "fileName": file.filename}}

# ==================== GRADIO FRONTEND ====================
def build_gradio_ui():
    current_user = {"username": "GiriÅŸ yapÄ±lmadÄ±"}
    current_token = None
    current_agents = []
    # Her model iÃ§in ayrÄ± chat history tut
    model_chat_histories = {
        "GPT": [],
        "BERT-CASED": [],
        "BERT-SENTIMENT": []
    }
    
    # Custom CSS - Koyu tema, profesyonel
    custom_css = """
    .gradio-container {
        background: #1a1a1a !important;
        font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, 'Helvetica Neue', Arial, sans-serif !important;
    }
    .gr-button-primary {
        background: #3b82f6 !important;
        color: white !important;
        border: none !important;
        font-weight: 500 !important;
    }
    .gr-button-primary:hover {
        background: #2563eb !important;
    }
    .gr-button {
        background: #4b5563 !important;
        color: white !important;
        border: none !important;
    }
    .gr-button:hover {
        background: #6b7280 !important;
    }
    .gr-textbox input, .gr-textbox textarea {
        background: #2d2d2d !important;
        color: #e5e5e5 !important;
        border: 1px solid #404040 !important;
    }
    .gr-textbox input:focus, .gr-textbox textarea:focus {
        border-color: #3b82f6 !important;
        outline: none !important;
    }
    .gr-textbox label {
        color: #d1d5db !important;
        font-weight: 500 !important;
    }
    .gr-markdown {
        color: #e5e5e5 !important;
    }
    .gr-markdown h1, .gr-markdown h2, .gr-markdown h3 {
        color: #ffffff !important;
    }
    .gr-radio label {
        color: #d1d5db !important;
    }
    .gr-dropdown {
        background: #2d2d2d !important;
        color: #e5e5e5 !important;
        border: 1px solid #404040 !important;
    }
    .gr-tabs {
        background: #1a1a1a !important;
    }
    .gr-tab {
        background: #2d2d2d !important;
        color: #d1d5db !important;
    }
    .gr-tab.selected {
        background: #3b82f6 !important;
        color: white !important;
    }
    .gr-chatbot {
        background: #2d2d2d !important;
    }
    .gr-chatbot .message {
        color: #e5e5e5 !important;
    }
    body {
        background: #1a1a1a !important;
    }
    """
    
    def login_fn(username, password):
        if not username or not password:
            return (
                "KullanÄ±cÄ± adÄ± ve ÅŸifre gerekli", 
                gr.update(visible=True),   # Login tab gÃ¶rÃ¼nÃ¼r
                gr.update(visible=False),  # Chat tab gizli
                gr.update(visible=False),  # Companies tab gizli
                gr.update(visible=False),  # Agents tab gizli
                gr.update()                # Agent dropdown boÅŸ
            )
        
        try:
            import requests
            # Backend'in hazÄ±r olmasÄ± iÃ§in kÄ±sa bir bekleme
            time.sleep(1)
            resp = requests.post(
                "http://localhost:3000/api/auth/login",
                json={"username": username, "password": password},
                timeout=10
            )
            if resp.status_code == 200:
                data = resp.json()
                nonlocal current_user, current_token, current_agents
                current_user = data["data"]
                current_token = data["token"]
                # Agent listesini gÃ¼ncelle
                try:
                    agent_resp = requests.get(
                        "http://localhost:3000/api/agents",
                        headers={"Authorization": f"Bearer {current_token}"},
                        timeout=5
                    )
                    if agent_resp.status_code == 200:
                        current_agents = agent_resp.json()["data"]
                        agent_choices = [f"{a['name']}" for a in current_agents]
                    else:
                        current_agents = []
                        agent_choices = []
                except Exception as e:
                    print(f"[!] Agent listesi alÄ±namadÄ±: {e}")
                    current_agents = []
                    agent_choices = []
                
                return (
                    f"âœ… GiriÅŸ baÅŸarÄ±lÄ±: {current_user.get('username', '')}",
                    gr.update(visible=False),  # Login tab'Ä± gizle
                    gr.update(visible=True),   # Chat tab'Ä± gÃ¶ster
                    gr.update(visible=current_user.get("isSuperAdmin", False)),  # Companies tab
                    gr.update(visible=True)    # Analytics tab
                )
            else:
                error_msg = resp.json().get("detail", "GiriÅŸ baÅŸarÄ±sÄ±z")
                return (
                    f"âŒ {error_msg}", 
                    gr.update(visible=True),   # Login tab gÃ¶rÃ¼nÃ¼r
                    gr.update(visible=False),  # Chat tab gizli
                    gr.update(visible=False),  # Companies tab gizli
                    gr.update(visible=False)   # Analytics tab gizli
                )
        except requests.exceptions.ConnectionError:
            return (
                "âŒ Backend'e baÄŸlanÄ±lamadÄ±. LÃ¼tfen birkaÃ§ saniye bekleyip tekrar deneyin.", 
                gr.update(visible=True),   # Login tab gÃ¶rÃ¼nÃ¼r
                gr.update(visible=False),  # Chat tab gizli
                gr.update(visible=False),  # Companies tab gizli
                gr.update(visible=False)   # Analytics tab gizli
            )
        except Exception as e:
            return (
                f"âŒ Hata: {str(e)}", 
                gr.update(visible=True),   # Login tab gÃ¶rÃ¼nÃ¼r
                gr.update(visible=False),  # Chat tab gizli
                gr.update(visible=False),  # Companies tab gizli
                gr.update(visible=False)   # Analytics tab gizli
            )
    
    def get_model_key(model_name):
        """Model adÄ±ndan kÄ±sa key Ã§Ä±kar"""
        if "gpt2-turkish" in model_name.lower() or "gpt-2" in model_name.lower() or "gpt2" in model_name.lower():
            return "GPT"
        elif "bert-base-turkish" in model_name.lower() and "sentiment" not in model_name.lower():
            return "BERT-CASED"
        elif "sentiment" in model_name.lower():
            return "BERT-SENTIMENT"
        return "GPT"
    
    def chat_fn(message, history, model):
        """Chat fonksiyonu - agent_id artÄ±k gerekmiyor, sabit index kullanÄ±yoruz"""
        if not message or not message.strip():
            return history or [], ""
        
        if not current_token:
            return history or [], "Ã–nce giriÅŸ yapÄ±n"
        
        # Model key'ini al
        model_key = get_model_key(model)
        
        # Bu model iÃ§in history'yi al (yoksa baÅŸlat)
        if history is None:
            history = model_chat_histories.get(model_key, []).copy()
        else:
            # History'yi gÃ¼ncelle
            model_chat_histories[model_key] = history.copy()
        
        # KullanÄ±cÄ± mesajÄ±nÄ± ekle
        history.append({"role": "user", "content": message})
        
        try:
            import requests
            # Sabit agent_id kullan (default index)
            # Backend'de default agent oluÅŸturulmuÅŸ olmalÄ± veya direkt RAG engine kullan
            resp = requests.post(
                "http://localhost:3000/api/chat",
                json={"agent_id": "default", "query": message, "model": model_key},
                headers={"Authorization": f"Bearer {current_token}"},
                timeout=60
            )
            if resp.status_code == 200:
                data = resp.json()["data"]
                # Bot cevabÄ±nÄ± ekle
                answer = data.get("answer", "Cevap alÄ±namadÄ±")
                history.append({"role": "assistant", "content": answer})
                # History'yi kaydet
                model_chat_histories[model_key] = history.copy()
                return history, ""
            else:
                error_msg = f"Hata: {resp.status_code}"
                try:
                    error_detail = resp.json().get("detail", error_msg)
                    error_msg = f"Hata {resp.status_code}: {error_detail}"
                except:
                    pass
                history.append({"role": "assistant", "content": error_msg})
                model_chat_histories[model_key] = history.copy()
                return history, ""
        except requests.exceptions.ConnectionError:
            error_msg = "Backend'e baÄŸlanÄ±lamadÄ±. LÃ¼tfen birkaÃ§ saniye bekleyip tekrar deneyin."
            history.append({"role": "assistant", "content": error_msg})
            model_chat_histories[model_key] = history.copy()
            return history, ""
        except Exception as e:
            import traceback
            error_detail = traceback.format_exc()
            print(f"[!] Chat hatasÄ±: {error_detail}")
            error_msg = f"Hata: {str(e)}"
            history.append({"role": "assistant", "content": error_msg})
            model_chat_histories[model_key] = history.copy()
            return history, ""
    
    def update_chat_history(model):
        """Model deÄŸiÅŸtiÄŸinde o modelin history'sini gÃ¶ster"""
        model_key = get_model_key(model)
        history = model_chat_histories.get(model_key, [])
        return history
    
    def create_agent_fn(name, embedding_model, data_source_type, data_source, uploaded_file, progress_output):
        print(f"[DEBUG] create_agent_fn Ã§aÄŸrÄ±ldÄ±: name={name}, uploaded_file={uploaded_file}")
        try:
            if not current_token:
                print("[DEBUG] Token yok")
                return (
                    "Ã–nce giriÅŸ yapÄ±n", 
                    gr.update(visible=False),
                    gr.update()  # Agent dropdown deÄŸiÅŸmez
                )
            
            if not name or not name.strip():
                print("[DEBUG] Agent adÄ± yok")
                return (
                    "Agent adÄ± gerekli", 
                    gr.update(visible=False),
                    gr.update()  # Agent dropdown deÄŸiÅŸmez
                )
            
            # Progress output'u gÃ¶ster
            progress_msg = "ğŸ“¤ Ä°ÅŸlem baÅŸlatÄ±lÄ±yor..."
            progress_update = gr.update(visible=True, value=progress_msg)
            print(f"[DEBUG] Progress mesajÄ±: {progress_msg}")
            
            # Dosya upload edildiyse Ã¶nce upload endpoint'ine gÃ¶nder
            final_data_source = data_source
            
            if uploaded_file is not None:
                try:
                    import requests
                    # Gradio File component bir dict dÃ¶ndÃ¼rÃ¼yor, path'i al
                    if isinstance(uploaded_file, dict):
                        file_path = uploaded_file.get('name') or uploaded_file.get('path')
                    else:
                        file_path = str(uploaded_file)
                    
                    file_name = Path(file_path).name
                    file_size = Path(file_path).stat().st_size / (1024 * 1024)  # MB
                    progress_msg = f"ğŸ“¤ **Dosya yÃ¼kleniyor:** {file_name} ({file_size:.1f} MB)"
                    progress_update = gr.update(visible=True, value=progress_msg)
                    
                    # DosyayÄ± upload et
                    with open(file_path, "rb") as f:
                        files = {"file": (file_name, f, "application/octet-stream")}
                        headers = {"Authorization": f"Bearer {current_token}"}
                        
                        upload_resp = requests.post(
                            "http://localhost:3000/api/upload",
                            files=files,
                            headers=headers,
                            timeout=300
                        )
                        
                        if upload_resp.status_code == 200:
                            upload_data = upload_resp.json()["data"]
                            final_data_source = upload_data["filePath"]
                            progress_msg = f"âœ… **Dosya yÃ¼klendi:** {file_name}\n\nğŸ”„ **Embedding baÅŸlÄ±yor...**\nğŸ“ Dosya parse ediliyor ve parÃ§alara bÃ¶lÃ¼nÃ¼yor..."
                            progress_update = gr.update(visible=True, value=progress_msg)
                        else:
                            return (
                                f"âŒ Dosya yÃ¼kleme hatasÄ±: {upload_resp.json().get('detail', 'Bilinmeyen hata')}", 
                                gr.update(visible=False),
                                gr.update()  # Agent dropdown deÄŸiÅŸmez
                            )
                except Exception as e:
                    import traceback
                    print(f"[!] Upload hatasÄ±: {traceback.format_exc()}")
                    return (
                        f"âŒ Dosya yÃ¼kleme hatasÄ±: {str(e)}", 
                        gr.update(visible=False),
                        gr.update()  # Agent dropdown deÄŸiÅŸmez
                    )
            elif not data_source or not data_source.strip():
                return (
                    "URL veya dosya gerekli", 
                    gr.update(visible=False),
                    gr.update()  # Agent dropdown deÄŸiÅŸmez
                )
            else:
                progress_msg = "ğŸ”„ Agent oluÅŸturuluyor ve veriler iÅŸleniyor...\n(Bu iÅŸlem dosya boyutuna gÃ¶re birkaÃ§ dakika sÃ¼rebilir)"
                progress_update = gr.update(visible=True, value=progress_msg)
            
            try:
                import requests
                progress_msg = "ğŸ”„ **Agent oluÅŸturuluyor...**\n\nğŸ“Š **Ä°ÅŸlem AdÄ±mlarÄ±:**\n1. Dosya parse ediliyor...\n2. Metin parÃ§alara bÃ¶lÃ¼nÃ¼yor...\n3. Embedding yapÄ±lÄ±yor (bu en uzun sÃ¼ren adÄ±m)...\n   - Batch'ler halinde iÅŸleniyor\n   - Her batch'te progress terminal'de gÃ¶rÃ¼necek\n4. FAISS index oluÅŸturuluyor...\n\nâ³ Bu iÅŸlem dosya boyutuna gÃ¶re birkaÃ§ dakika sÃ¼rebilir.\nğŸ“ Terminal'de detaylÄ± progress log'larÄ± gÃ¶rebilirsiniz (Batch X/Y, %YÃ¼zde)."
                progress_update = gr.update(visible=True, value=progress_msg)
                
                resp = requests.post(
                    "http://localhost:3000/api/agents",
                    json={
                        "name": name,
                        "embedding_model": embedding_model,
                        "data_source_type": data_source_type if not uploaded_file else "file",
                        "data_source": final_data_source
                    },
                    headers={"Authorization": f"Bearer {current_token}"},
                    timeout=600  # 10 dakika timeout
                )
                if resp.status_code == 200:
                    data = resp.json()
                    agent_data = data.get("data", {})
                    ingestion_info = data.get("ingestion_info", {})
                    chunks = agent_data.get("chunkCount", ingestion_info.get("chunks", 0))
                    agent_id = agent_data.get("id", "bilinmiyor")
                    
                    # Agent listesini gÃ¼ncelle ve dropdown'Ä± gÃ¼ncelle
                    agent_choices_updated = []
                    try:
                        agent_resp = requests.get(
                            "http://localhost:3000/api/agents",
                            headers={"Authorization": f"Bearer {current_token}"},
                            timeout=5
                        )
                        if agent_resp.status_code == 200:
                            nonlocal current_agents
                            current_agents = agent_resp.json()["data"]
                            agent_choices_updated = [f"{a['name']}" for a in current_agents]
                    except:
                        pass
                    
                    index_name = agent_data.get("indexName", "N/A")
                    
                    success_msg = f"""âœ… **Agent BaÅŸarÄ±yla OluÅŸturuldu!**

ğŸ“‹ **Agent Bilgileri:**
- **Ad:** {name}
- **ID:** `{agent_id}`
- **Index AdÄ±:** `{index_name}`
- **Ä°ÅŸlenen ParÃ§a SayÄ±sÄ±:** {chunks}
- **Embedding Model:** `{embedding_model}`

ğŸ“ **Index Konumu:**
`python_services/data/faiss_index/{index_name}/`

ğŸ’¬ **KullanÄ±m:**
Chat sayfasÄ±ndan agent'Ä± seÃ§ip sorularÄ±nÄ±zÄ± sorabilirsiniz!

ğŸ“Š **Terminal'de gÃ¶rebileceÄŸiniz log'lar:**
- Dosya parse iÅŸlemi
- Chunk oluÅŸturma
- Batch'ler halinde embedding (Ã¶rn: Batch 1/5 - %20, Batch 2/5 - %40...)
- Index kaydetme"""
                    
                    # Agent dropdown'Ä±nÄ± gÃ¼ncelle
                    return (
                        success_msg, 
                        gr.update(visible=False),
                        gr.update(choices=agent_choices_updated, value=agent_choices_updated[0] if agent_choices_updated else None)
                    )
                else:
                    return (
                        f"âŒ Hata: {resp.json().get('detail', 'Bilinmeyen hata')}", 
                        gr.update(visible=False),
                        gr.update()  # Agent dropdown deÄŸiÅŸmez
                    )
            except requests.exceptions.Timeout:
                return (
                    "âŒ Ä°ÅŸlem zaman aÅŸÄ±mÄ±na uÄŸradÄ±. Dosya Ã§ok bÃ¼yÃ¼k olabilir.", 
                    gr.update(visible=False),
                    gr.update()  # Agent dropdown deÄŸiÅŸmez
                )
            except Exception as e:
                import traceback
                error_detail = traceback.format_exc()
                print(f"[!] Agent oluÅŸturma hatasÄ±: {error_detail}")
                return (
                    f"âŒ Hata: {str(e)}", 
                    gr.update(visible=False),
                    gr.update()  # Agent dropdown deÄŸiÅŸmez
                )
        except Exception as e:
            import traceback
            error_detail = traceback.format_exc()
            print(f"[!] Agent oluÅŸturma genel hatasÄ±: {error_detail}")
            return (
                f"âŒ Genel hata: {str(e)}", 
                gr.update(visible=False),
                gr.update()  # Agent dropdown deÄŸiÅŸmez
            )
    
    def create_company_fn(name, email):
        if not current_token:
            return "Ã–nce giriÅŸ yapÄ±n"
        try:
            import requests
            resp = requests.post(
                "http://localhost:3000/api/admin/companies",
                json={"name": name, "email": email},
                headers={"Authorization": f"Bearer {current_token}"},
                timeout=10
            )
            if resp.status_code == 200:
                data = resp.json()["data"]
                return f"âœ… Åirket oluÅŸturuldu!\nKullanÄ±cÄ±: {data['username']}\nÅifre: {data['password']}"
            else:
                return f"âŒ Hata: {resp.json().get('detail', 'Bilinmeyen hata')}"
        except Exception as e:
            return f"âŒ Hata: {str(e)}"
    
    # Custom CSS - Koyu tema, profesyonel
    custom_css = """
    .gradio-container {
        background: #1a1a1a !important;
        font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, 'Helvetica Neue', Arial, sans-serif !important;
    }
    .gr-button-primary {
        background: #3b82f6 !important;
        color: white !important;
        border: none !important;
        font-weight: 500 !important;
    }
    .gr-button-primary:hover {
        background: #2563eb !important;
    }
    .gr-button {
        background: #4b5563 !important;
        color: white !important;
        border: none !important;
    }
    .gr-button:hover {
        background: #6b7280 !important;
    }
    .gr-textbox input, .gr-textbox textarea {
        background: #2d2d2d !important;
        color: #e5e5e5 !important;
        border: 1px solid #404040 !important;
    }
    .gr-textbox input:focus, .gr-textbox textarea:focus {
        border-color: #3b82f6 !important;
        outline: none !important;
    }
    .gr-textbox label {
        color: #d1d5db !important;
        font-weight: 500 !important;
    }
    .gr-markdown {
        color: #e5e5e5 !important;
    }
    .gr-markdown h1, .gr-markdown h2, .gr-markdown h3 {
        color: #ffffff !important;
    }
    .gr-radio label {
        color: #d1d5db !important;
    }
    .gr-dropdown {
        background: #2d2d2d !important;
        color: #e5e5e5 !important;
        border: 1px solid #404040 !important;
    }
    .gr-tabs {
        background: #1a1a1a !important;
    }
    .gr-tab {
        background: #2d2d2d !important;
        color: #d1d5db !important;
    }
    .gr-tab.selected {
        background: #3b82f6 !important;
        color: white !important;
    }
    .gr-chatbot {
        background: #2d2d2d !important;
    }
    .gr-chatbot .message {
        color: #e5e5e5 !important;
    }
    body {
        background: #1a1a1a !important;
    }
    """
    
    # CSS'i Blocks constructor'a ekle (Gradio versiyonuna gÃ¶re)
    try:
        # Yeni versiyonlar iÃ§in css parametresi Blocks'ta
        app = gr.Blocks(title="RAG SaaS Platform", css=custom_css)
    except TypeError:
        # Eski versiyonlar iÃ§in css yok
        app = gr.Blocks(title="RAG SaaS Platform")
    
    # UI'Ä± oluÅŸtur
    with app:
        gr.Markdown("# RAG SaaS Platform")
        
        with gr.Tab("GiriÅŸ", visible=True) as login_tab:
            with gr.Row():
                with gr.Column():
                    login_user = gr.Textbox(label="KullanÄ±cÄ± AdÄ±", value="admin@ragplatform.com")
                    login_pass = gr.Textbox(label="Åifre", type="password", value="Admin123!@#")
                    login_btn = gr.Button("GiriÅŸ Yap", variant="primary")
                    login_status = gr.Markdown()
        
        with gr.Tab("Chat", visible=False) as chat_tab:
            with gr.Row():
                with gr.Column():
                    model_radio = gr.Radio(
                        [
                            "dbmdz/gpt2-turkish (GPT-2 TÃ¼rkÃ§e)",
                            "bert-base-turkish-cased (BERT TÃ¼rkÃ§e)",
                            "savasy/bert-base-turkish-sentiment-cased (BERT Sentiment)"
                        ],
                        value="dbmdz/gpt2-turkish (GPT-2 TÃ¼rkÃ§e)",
                        label="Model SeÃ§",
                        info="Her model iÃ§in ayrÄ± chat geÃ§miÅŸi tutulur"
                    )
                    chatbot = gr.Chatbot(label="Chat", height=500, type="messages", allow_tags=False)
                    msg_input = gr.Textbox(label="Mesaj", placeholder="Sorunuzu yazÄ±n...")
                    send_btn = gr.Button("GÃ¶nder", variant="primary")
                    
                    # Model deÄŸiÅŸtiÄŸinde history'yi gÃ¼ncelle
                    model_radio.change(
                        update_chat_history,
                        inputs=[model_radio],
                        outputs=[chatbot]
                    )
                    
                    send_btn.click(
                        chat_fn,
                        inputs=[msg_input, chatbot, model_radio],
                        outputs=[chatbot, msg_input]
                    )
                    msg_input.submit(
                        chat_fn,
                        inputs=[msg_input, chatbot, model_radio],
                        outputs=[chatbot, msg_input]
                    )
        
        with gr.Tab("Analytics", visible=False) as analytics_tab:
            with gr.Row():
                with gr.Column():
                    gr.Markdown("## Model Performance Metrics")
                    gr.Markdown("Her model iÃ§in Exact Match, F1 Score, ROUGE-L ve Cosine Similarity metrikleri hesaplanÄ±r.")
                    
                    evaluate_btn = gr.Button("Metrikleri Hesapla ve Grafikle", variant="primary")
                    metrics_output = gr.Markdown()
                    metrics_plot = gr.Image(label="Metrics Comparison Chart")
                    
                    def evaluate_models():
                        """TÃ¼m modelleri deÄŸerlendir ve grafik oluÅŸtur"""
                        try:
                            # Test verisini yÃ¼kle (CSV'den)
                            import pandas as pd
                            csv_path = "/content/sample_data/test_cleaned.csv"
                            if not os.path.exists(csv_path):
                                csv_path = str(BASE_DIR / "python_services" / "data" / "test_cleaned.csv")
                            
                            if not os.path.exists(csv_path):
                                return "âŒ Test CSV dosyasÄ± bulunamadÄ±. LÃ¼tfen /content/sample_data/test_cleaned.csv konumuna yerleÅŸtirin.", None
                            
                            df = pd.read_csv(csv_path)
                            # question_content ve question_answer sÃ¼tunlarÄ±nÄ± kullan
                            if "question_content" not in df.columns or "question_answer" not in df.columns:
                                return "âŒ CSV'de 'question_content' ve 'question_answer' sÃ¼tunlarÄ± bulunamadÄ±.", None
                            
                            questions = df["question_content"].dropna().tolist()[:50]  # Ä°lk 50 soru
                            references = df["question_answer"].dropna().tolist()[:50]
                            
                            if len(questions) != len(references):
                                return f"âŒ Soru ve cevap sayÄ±larÄ± eÅŸleÅŸmiyor: {len(questions)} soru, {len(references)} cevap", None
                            
                            evaluator = MetricsEvaluator()
                            rag = SimpleRAGEngine(index_name="default")
                            
                            models_to_test = ["GPT", "BERT-CASED", "BERT-SENTIMENT"]
                            all_results = {}
                            
                            for model_key in models_to_test:
                                print(f"[*] {model_key} modeli test ediliyor...")
                                predictions = []
                                
                                for q in questions:
                                    try:
                                        result = rag.query(q, model_type=model_key)
                                        predictions.append(result.get("answer", ""))
                                    except Exception as e:
                                        print(f"[!] {model_key} iÃ§in hata: {e}")
                                        predictions.append("")
                                
                                # Metrikleri hesapla
                                metrics = evaluator.evaluate(predictions, references)
                                all_results[model_key] = metrics
                            
                            # Grafik oluÅŸtur
                            plots_dir = BASE_DIR / "frontend_gradio" / "assets" / "plots"
                            plots_dir.mkdir(parents=True, exist_ok=True)
                            plot_path = plots_dir / "metrics_comparison.png"
                            
                            fig = evaluator.plot_metrics(all_results, save_path=str(plot_path))
                            
                            # SonuÃ§larÄ± formatla
                            result_text = "## ğŸ“Š Model Performance Results\n\n"
                            for model_key, metrics in all_results.items():
                                result_text += f"### {model_key}\n"
                                result_text += f"- **Exact Match:** {metrics['exact_match']['mean']:.3f}\n"
                                result_text += f"- **F1 Score:** {metrics['f1']['mean']:.3f}\n"
                                result_text += f"- **ROUGE-L:** {metrics['rouge_l']['mean']:.3f}\n"
                                result_text += f"- **Cosine Similarity:** {metrics['cosine_similarity']['mean']:.3f}\n\n"
                            
                            return result_text, str(plot_path)
                        except Exception as e:
                            import traceback
                            error_detail = traceback.format_exc()
                            print(f"[!] Evaluation hatasÄ±: {error_detail}")
                            return f"âŒ Hata: {str(e)}\n\n```\n{error_detail}\n```", None
                    
                    evaluate_btn.click(
                        evaluate_models,
                        outputs=[metrics_output, metrics_plot]
                    )
        
        with gr.Tab("Åirket YÃ¶netimi", visible=False) as companies_tab:
            with gr.Row():
                with gr.Column():
                    comp_name = gr.Textbox(label="Åirket AdÄ±")
                    comp_email = gr.Textbox(label="Email (opsiyonel)")
                    create_comp_btn = gr.Button("Åirket OluÅŸtur", variant="primary")
                    comp_status = gr.Markdown()
                    
                    create_comp_btn.click(
                        create_company_fn,
                        inputs=[comp_name, comp_email],
                        outputs=[comp_status]
                    )
        
        login_btn.click(
            login_fn,
            inputs=[login_user, login_pass],
            outputs=[login_status, login_tab, chat_tab, companies_tab, analytics_tab]
        )
    
    return app, custom_css

# ==================== STARTUP ====================
def run_backend():
    uvicorn.run(backend_app, host="0.0.0.0", port=3000, log_level="error")

def run_frontend():
    result = build_gradio_ui()
    # Debug: result tipini kontrol et
    if isinstance(result, tuple):
        app, custom_css = result
    else:
        # EÄŸer tuple deÄŸilse, eski versiyon gibi davran
        app = result
        custom_css = ""
        print("[!] UyarÄ±: build_gradio_ui() tuple dÃ¶ndÃ¼rmedi, CSS kullanÄ±lamayacak")
    
    is_colab = False
    try:
        import google.colab
        is_colab = True
    except:
        pass
    
    share_value = is_colab or os.getenv("GRADIO_SHARE", "").lower() == "true"
    print(f"\n[*] Gradio baÅŸlatÄ±lÄ±yor (share={share_value})...")
    
    # CSS zaten Blocks constructor'da, launch'ta gerek yok
    try:
        app.launch(
            server_name="0.0.0.0",
            server_port=7860,
            share=share_value,
            show_error=True
        )
    except Exception as e:
        # Fallback
        app.launch(
            server_name="0.0.0.0",
            server_port=7860,
            share=share_value
        )

if __name__ == "__main__":
    print("\n[4/4] Backend baÅŸlatÄ±lÄ±yor...")
    backend_thread = threading.Thread(target=run_backend, daemon=True)
    backend_thread.start()
    time.sleep(5)  # Backend'in baÅŸlamasÄ± iÃ§in bekle
    print("âœ… Backend: http://localhost:3000")
    
    print("\n[5/5] Frontend baÅŸlatÄ±lÄ±yor...")
    print("\n" + "=" * 60)
    print("ğŸ”‘ GiriÅŸ: admin@ragplatform.com / Admin123!@#")
    print("=" * 60)
    print("\nâ³ Gradio public URL oluÅŸturuluyor...")
    print("   (Bu iÅŸlem 10-20 saniye sÃ¼rebilir)\n")
    
    # Frontend'i ana thread'de Ã§alÄ±ÅŸtÄ±r (blocking)
    run_frontend()
                    